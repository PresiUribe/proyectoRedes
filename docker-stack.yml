version: "3.8"

services:
  db:
    image: mysql:8.0
    container_name: db
    environment:
      # Usamos contraseÃ±a fija; puedes cambiarlo o pasarlo por secret
      MYSQL_ROOT_PASSWORD: mysql
    volumes:
      - db_data:/var/lib/mysql
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - ./Airbnb_Texas_Rentals.csv:/var/lib/mysql-files/propiedades.csv
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    networks:
      - back

  usuario_ms:
    image: usuario_ms:latest
    depends_on: [db]
    environment:
      DB_HOST: db
      DB_USER: root
      DB_PASSWORD: mysql
      DB_NAME: usuariosMS
    deploy:
      replicas: 3
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
    networks:
      - back

  reservas_ms:
    image: reservas_ms:latest
    depends_on: [db]
    environment:
      DB_HOST: db
      DB_USER: root
      DB_PASSWORD: mysql
      DB_NAME: reservasMS
    deploy:
      replicas: 3
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
    networks:
      - back

  propiedades_ms:
    image: propiedades_ms:latest
    depends_on: [db]
    environment:
      DB_HOST: db
      DB_USER: root
      DB_PASSWORD: mysql
      DB_NAME: propiedadesMS
    deploy:
      replicas: 3
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
    networks:
      - back

  pagos_ms:
    image: pagos_ms:latest
    depends_on: [db]
    environment:
      DB_HOST: db
      DB_USER: root
      DB_PASSWORD: mysql
      DB_NAME: pagosMS
    deploy:
      replicas: 3
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
    networks:
      - back

  haproxy:
    image: haproxy:2.6-alpine
    depends_on:
      - usuario_ms
      - reservas_ms
      - propiedades_ms
      - pagos_ms
    ports:
      - "80:80"
      - "8404:8404"
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
      update_config:
        parallelism: 1
        delay: 5s
    networks:
      - back

  haproxy-exporter:
    image: prom/haproxy-exporter:latest
    command:
      - --haproxy.scrape-uri=http://admin:secret@haproxy:8404/stats;csv
    ports:
      - "9101:9101"
    networks:
      - back

  prometheus:
    image: prom/prometheus:latest
    depends_on:
      - haproxy
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    networks:
      - back

  grafana:
    image: grafana/grafana:latest
    container_name: proyecto_grafana
    depends_on:
      - prometheus
    ports:
      - "3003:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    networks:
      - back

  frontend:
    image: nginx:alpine
    volumes:
      - ./front:/usr/share/nginx/html:ro
      - ./front/nginx.conf:/etc/nginx/conf.d/default.conf:ro   # opcional
    ports:
      - "8080:80"
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
    networks:
      - back


  spark-master:
    image: bitnami/spark:3.3.2
    command: start-master.sh
    ports:
      - "7077:7077"
      - "8082:8082"
    networks:
      - back

  spark-worker:
    image: bitnami/spark:3.3.2
    depends_on:
      - spark-master
    command: start-worker.sh spark://spark-master:7077
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
    networks:
      - back

  spark-job:
    image: myregistry/streaming-spark:latest
    depends_on:
      - spark-master
      - spark-worker
    command: >
      /opt/bitnami/spark/bin/spark-submit
        --master spark://spark-master:7077
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2
        /opt/bitnami/spark/app/streaming_job.py
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
    networks:
      - back


networks:
  back:
    driver: overlay

volumes:
  db_data:
